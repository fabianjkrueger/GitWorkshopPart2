## Data Science Track

This semester's project phase being a part of the "Beautify Berlin" team has truly been an amazing journey for me. In fact, not everything worked as we had planned in the beginning and there were quite a few challenges, but in the end, we managed to work our way around them to find our way to jointly and successfully develop our prototype of this wonderful app. In the following part, I will describe to you the past weeks from the perspective of data science (DS).

## Ideation

While it is obvious what a web developer may do in order to assemble the app and that, for example, a user experience designer is needed to lay out plans for the project and think of features a potential user would love, a possible contribution by a data scientist was not as obvious from the start. This is especially true when considering that the implementation of a machine learning algorithm was part of the deliverables required for successful completion of the course. Our team wanted to bring together owners of electric boxes, artists and regular people of Berlin to beautify unsightly places in the city by installing street art on electric boxes. Of course, knowing how many electric boxes there are, where they are and who owns them is necessary for this. But this is not actually the kind of data a machine learning algorithm can be trained on to predict information meaningful for the users of the app. During the first four weeks, I browsed the internet on the look for data which could be used for our application. Apart from that, I helped forming the general scope and ideation of the project along my team members. Unfortunately, I was not able to find useful data sets. The only one available basically only contained an ID for each electric box and the geographical coordinates for most of them. Obviously, without data, I also had not a clue how I could contribute to the project. The facts that I was the only DS techie assigned to the project and that our mentor ghosted us made it even more difficult, because I did not have anyone to collaborate and to have a joint ideation session for the DS part with. At this point I was not even sure anymore if I would be able to complete the course and qualify for the certificate. But of course, I still had the help of my team mates, who never let me down even though they were from different tracks and did not necessarily know about what a machine learning algorithm was able to do. After the midterm presentation we talked about the problem in our weekly meeting. Together, we decided that we would take another approach. We turned the lack of real world data sets, which at the start we tought of as a disadvantage into an opportunity. We decided to generate synthetic user data of artworks already completed by the Beautify Berlin community and use it to train a machine learning algorithm to predict whether a future application for creating an artwork would likely be approved or not. We decided to take into account among others features like the district in which the artwork the electric box could be found, the type of street art, the artists level of experience and a user rating done by the community on a draft of the artwork. Once the app was launched and there would be sufficient user data, an algorithm like this could help artists to assess whether the effort of applying would be worth it based on real world data.

### Synthetic Data Generation

I immediately began compiling useful features for this data set which would allow me to implement such an algorithm. I decided for features according to what I just wrote previously: Basic information about the artist, the artwork and the location. Each artwork was generated by having each of its features randomly picked by numpy from a list of possible options for that specific feature. While some features, like the district in which the artwork could be found, were uniformly distributed, others were not. For example, the type of the artwork (e.g., painting or graffiti) was drawn from a non-uniformly distributed list: The chance of it being a painting was 52.5%, while the chance of it being a graffito was just 32.5%. In order to introduce some more challenges, I introduced some missing values. To be more specific, each artwork had a ten percent chance of not being rated by users. That way, a data set of 750 artworks was generated. Apart from generating the features, I also labeled the data set myself. I acted like I was the owner of the boxes and for each artwork I had a look at the features and then decided whether I would approve that artwork or not. I introduced some bias that may not be true, but I needed to do it so that the algorithm was able to learn anything. (Disclaimer: These biases refer, for example, to which types of artwork (e.g., paintings or graffiti) are preferred over others by the owners or to that owners are more likely to approve artwork in some districts than in some others. These biases do not necessarily reflect the truth or the opinions of Beautify Berlin and TechLabs. They were introduced so that an ml algorithm is able to learn from the data set.) In the end, I had a list of 750 applications for artworks plus their outcomes.

The final data set looked like this:

<p align="center">
<img src="/DS_images/artworks_raw.png" />
</p>

### Data Preparation

As first step of the data handling and preparation, I categorised the features. While some features were naturally numerical and continuous, some were represented by character strings. I turned them into numerical values and ranked them according to the likelyhood of improving chance of approval. Higher values hereby represented higher chances of approval and vice versa. This was necessary, because the algorithms imported from scikit learn taught to us are only able to handle numerical data. Next, I handled the missing values (user rating) by filling these missing values with the value zero. This is because a missing user rating lowered the chance of the artwork getting approved. Filling with zero thus fit well into the order of the categories and prevented losing about ten percent of observations.

Afterwards, the data looked like this:

<p align="center">
<img src="/DS_images/artworks_processed.png" />
</p>

### Data Visualisations

Even though I set the distributions of the features myself, I wanted to get an overview of the features and their distributions and convice myself that the generation of synthetic data worked out the way I planned. For this, I visualised the data using several types of graphs like bar plots (if there were more than two options) or pie charts (if there were just two options). Finally, I assembled a scatter plot matrix, which convinced me that the synthetic data set was suitable for ml, meaning that an algorithm was likely to be able to learn from it.

Below, you can see two examples. For further visualisations, please have a look at the Jupyter notebook "data_visualisations.ipynb"

<p align="center">
<img src="/DS_images/summary_statistics.png" />
</p>

<p align="center">
<img src="/DS_images/districts.png" />
</p>


### Machine Learning Algorithm

The goal of the algorithm was to predict the outcome of the application. I decided to use a decision tree classifier for this task. Firstly, because the task is a classification problem and secondly, because a decision tree classifier allows for the determination of information like feature importance. I determined the optimum depth of the classifier by determining highest average accuracy on the test set averaged over 1000 train-test-splits. This test resulted in 6 being the optimum depth. The resulting classifier I trained that way had an accuracy of 83%.

<p align="center">
<img src="/DS_images/depth.png" />
</p>

This is what the tree looks like: 
(For full solution, please have a look at the Jupyter notebook "approval_classifier.ipynb")

<p align="center">
<img src="/DS_images/tree.png" />
</p>

Remember that I did the labeling myself based on MY preferences and the biases that I decided to introduce. What that means is also that the algorithm basically learned about my personal preferences and the mentioned biases. To me, seeing the feature importances was the most fascinating part of the project. During labeling, I needed to look at the features and then decide whether I would approve that specific artwork or not. There were some features I did not pay a lot of attention to, like e.g., the number of artists involved. And there were also some features that were very important to me, like e.g., the user rating, the type of the artwork or what could be found on the box before. After training a decision tree classifier I visualised these feature importances. The graph reflected what has been important to me and in which order/and how much really precisely. That was incredibly astonishing to me and showed me just how good algorithms have become in learning from data.

<p align="center">
<img src="/DS_images/feature_importance.png" />
</p>

### Final Thoughts

Completing this TechLabs project tought me a large number of invaluable skills. Not only did I improve my abilities of data handling and coding in Python, but I also learned to cooperate and work with people from different tech stacks and backgrounds to collectively build an application which helps to tackle real world problems. After gaining all of this experience, I would still and actually even more than before love to take part in next semester's TechLabs AI track to deepen my knowledge about machine learning, deep learning and to improve in realising ideas in collaboration with a team from diverse tech stacks. Even though I would say that our project phase was successful, I also did a lot of mistakes and I would approach many things differently. But it is exactly these challenges, which by being overcome, help us becoming a better developer and teach us the most valuable lessons. I hope that I get the chance to implement what I learned from these mistakes in next semester's project phase.